# -*- coding: utf-8 -*-
"""
Created on Fri Mar  4 16:39:12 2022

@author: jessi
"""
import pandas as pd
import numpy as np
from scipy import stats
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

#%%
def export_results(samples, result_path, fold_type, y, pred, val_fold):
    '''
    samples: DataFrame;
                Dataframe that contains the the fold information
    result_path: string; 
                path to save the results
                
    fold_type: string; 
                'pair_folds, drug_folds, cl_folds'
                
    y: array; ground truth
    
    pred: array; predictions
    
    val_fold: list of int; 
                folds that are used as val/test fold
    '''
    test_samples = samples.loc[samples[fold_type].isin(val_fold)]
    
    result = pd.DataFrame(data={'cl': test_samples['SANGER_MODEL_ID'], 'drug': test_samples['DRUG_NAME'],
                                'ground_truth': y, 'pred': pred, 'fold': test_samples[fold_type]})

    result.to_csv(result_path, index=False)
    return result

def eval_metrics(result_file, fold_type):
    '''
    Description
    ------------------
    Calculate Pearson Correlation Coefficient, MSE, RMSE, and R2-score of the 5 CV folds by test fold
    
    Parameters
    ------------------
    result_file: pandas.DataFrame
        The file generated by the models, contains ground truth and predicted IC50 values and fold info
    
    fold_type: string ('pair', 'cl', 'drug')
        Defines the what fold is used for calculating the metrics
    '''
    pcc_ls = []
    spc_ls = []
    mse_ls = []
    rmse_ls = []
    r2_ls = []
    
    test_folds = result_file['fold'].unique()
    for i in test_folds:
        fold = result_file.loc[result_file['fold'] == i]
        y_true = fold['ground_truth'].values
        y_pred = fold['pred'].values
        pcc = stats.pearsonr(y_true, y_pred)[0]
        spc = stats.spearmanr(y_true, y_pred)[0]
        mse = mean_squared_error(y_true, y_pred)
        rmse = mean_squared_error(y_true, y_pred, squared=False)
        r2 = r2_score(y_true, y_pred)
        pcc_ls.append(pcc)
        spc_ls.append(spc)
        mse_ls.append(mse)
        rmse_ls.append(rmse)
        r2_ls.append(r2)

    all_metrics = pd.DataFrame(data={'fold_type': [fold_type]*test_folds.shape[0], 
                                     'avg_by': ['by_vector']*test_folds.shape[0],
                                     'fold': test_folds, 
                                     'pcc': pcc_ls, 'spc': spc_ls, 'mse': mse_ls, 'rmse': rmse_ls, 'r2': r2_ls})
    return all_metrics





def eval_metrics_by(result_file, fold_type, eval_by):
    '''
    Description
    ------------------
    Calculate Pearson Correlation Coefficient, Spearman correlation, MSE, RMSE, and R2-score of the 5 CV folds by test fold and drug/cell line
    
    Parameters
    ------------------
    result_file: pandas.DataFrame
        The file generated by the models, contains ground truth and predicted IC50 values and fold info
    
    fold_type: string ('pair', 'cl', 'drug')
        Defines the what fold is used for calculating the metrics
    
    eval_by: string ('cl', 'drug')
        Calcualte the evaluation metrics by cell line or drug, then average the results to generate
        final evaluation metrics for each test fold
    '''
    
    pcc_fold, spc_fold, mse_fold, rmse_fold, r2_fold = [],[],[],[],[]
    group_ls, fold_type_ls, i_ls, y_pred_shape_ls = [],[],[],[]
    
    test_folds = result_file['fold'].unique()
    
    for i in test_folds:
        pcc_ls, spc_ls, mse_ls, rmse_ls, r2_ls = [],[],[],[],[]
        
        fold = result_file.loc[result_file['fold'] == i]
        fold_group = fold.groupby([eval_by])
        groups = fold_group.groups.keys()
        
        for group in groups:
            y_true = fold_group.get_group(group)['ground_truth'].values
            y_pred = fold_group.get_group(group)['pred'].values
            
            not_single_prediction = y_true.shape[0] > 1 # if there's more than one prediction for that specific drug or cl
            constant_pred = np.all(y_pred == y_pred[0])
            not_constant_pred = np.logical_not(constant_pred) # 0 = constant predictions, 1 = not constant
            
            if not_single_prediction and constant_pred:
                group_ls.append(group)
                fold_type_ls.append(fold_type)
                i_ls.append(i)
                y_pred_shape_ls.append(y_pred.shape[0])
                
            
            if not_single_prediction and not_constant_pred:
                pcc = stats.pearsonr(y_true, y_pred)[0]
                spc = stats.spearmanr(y_true, y_pred)[0]
                mse = mean_squared_error(y_true, y_pred)
                rmse = mean_squared_error(y_true, y_pred, squared=False)
                r2 = r2_score(y_true, y_pred)
                pcc_ls.append(pcc)
                spc_ls.append(spc)
                mse_ls.append(mse)
                rmse_ls.append(rmse)
                r2_ls.append(r2)
        pcc_fold.append(np.mean(pcc_ls))
        spc_fold.append(np.mean(spc_ls))
        mse_fold.append(np.mean(mse_ls))
        rmse_fold.append(np.mean(rmse_ls))
        r2_fold.append(np.mean(r2_ls))
    
    constant_pred_df = pd.DataFrame(data={'drug/cl name': group_ls, 'CV method': fold_type_ls, 'fold': i_ls, 'num of samples': y_pred_shape_ls})
    all_metrics = pd.DataFrame(data={'fold_type': [fold_type]*test_folds.shape[0], 
                                     'avg_by': ['by_' + eval_by]*test_folds.shape[0],
                                     'fold': test_folds, 
                                     'pcc': pcc_fold, 'spc': spc_fold, 'mse': mse_fold, 'rmse': rmse_fold, 'r2': r2_fold})
    
    return constant_pred_df, all_metrics




